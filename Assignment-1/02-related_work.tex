\section{Related Work}

\subsection{Generating and Optimizing Code with LLMs }
In recent period, we have seen a huge tendency of both generating and optimizing codes using LLMs like GPT \cite{achiam2023gpt}, Codex \cite{chen2021evaluating}, CodeGen \cite{nijkamp2022codegen}, and InCoder \cite{fried2022incoder} which can accept requests in NL. Moreover, models like GitHub Copilots can perform code completion and Optimization based on the whole code-base and also provide suggestions \cite{barke2023grounded}.

Other LLM-based frameworks have also been developed to address different challenges in code generation and optimization. For instance, ClarifyGPT \cite{mu2024clarifygpt} enhances the code generation process by identifying ambiguities in user prompts and seeking clarifications, ensuring that generated code aligns closely with user intentions. CodePlan \cite{bairi2024codeplan} addresses the challenge of complex repository-level tasks by using a task-agnostic, neuro-symbolic framework that frames coding as a planning problem, synthesizing a multistep chain of edits through dependency analysis, change impact analysis, and adaptive planning with neural LLMs. SBLLM \cite{gao2024search} combines LLMs with search techniques for iterative code optimization, using representative sample selection, adaptive pattern retrieval, and genetic operator-inspired prompting to achieve code efficiency improvements. CoLadder \cite{yen2024coladder} provides a hierarchical structure for decomposing programming tasks, allowing programmers to better align their problem-solving intentions with LLM-generated code.
Unlike search-based optimization or task decomposition, our approach emphasizes providing programmers with real-time insights into their code's status through a structured interface that facilitates direct, segment-specific op-
timizations. MACEDON uniquely combines visualization of code state with targeted recommendations, enabling programmers to efficiently track and improve code quality through a seamless integration into their workflow.

\subsection{Evaluation of LLM-based Code Assistants }
Evaluating the quality of code produced by Large Language Models is essential for software development. Traditional benchmarks such as HUMANEVAL \cite{chen2021evaluating} and MBPP \cite{austin2021program} measure functional correctness by testing generated Python functions against predefined cases. To broaden this scope, MultiPL-E \cite{cassano2023multipl} and HumanEval-X \cite{zheng2023codegeex} translate these benchmarks into various languages for cross-language evaluation. Specialized challenges like AlphaCode \cite{li2022competition} utilize Codeforces to test complex problem-solving, while Spider \cite{yu2018spider} focuses on text-to-SQL tasks. EvalPlus \cite{liu2023your} enhances these methods by automatically generating larger sets of test inputs to increase testing coverage. Furthermore, SWE-bench shifts the focus toward real-world software engineering by requiring models to resolve GitHub issues within complex, multi-file codebases.

Unlike these static evaluations, MACEDON emphasizes real-time feedback and iterative refinement during the optimization process. While existing benchmarks primarily assess functional correctness through fixed tests, MACEDON provides a dynamic, user-centered framework. This allows programmers to evaluate code across multiple dimensions and refine it interactively using LLM-driven recommendations. By bridging the gap between one-off generation and continuous improvement, this approach offers a more practical solution for the ongoing nature of real-world programming tasks.

\subsection{Traditional Code Analysis and Optimization Tools }
Traditional code improvement has evolved through performance optimization and refactoring. Performance analysis tools such as HPCToolkit \cite{adhianto2010hpctoolkit} and Speedoo \cite{chen2018speedoo} identify bottlenecks using runtime data, while regression detection systems \cite{pantiuchina2020developers} maintain efficiency. However, these tools often require separate execution phases outside the primary development workflow. Similarly, static analysis and refactoring tools like JDeodorant \cite{fokaefs2007jdeodorant}, and IntelliJ \cite{pomian2024together} automate transformations based on established patterns . Despite their utility in maintaining consistency, they lack the contextual depth and semantic understanding necessary for complex, domain-specific tasks that depend on programmer intent.

Recent machine learning advances have introduced neural program repair and learning-based performance optimization. Research into mixed-initiative IDEs, such as Grounded Copilot \cite{barke2023grounded} and in-IDE generation , explores AI assistance but focuses primarily on generation rather than comprehensive evaluation or optimization workflows. As noted by quantitative assessments of development techniques , a significant challenge remains in providing real-time, multi-dimensional insights into a code's state to help programmers craft effective strategies.

Our work, MACEDON, addresses these gaps by merging real-time code evaluation with interactive optimization. Unlike traditional performance tools that operate in separate phases, MACEDON provides immediate feedback during the coding process. While static tools rely on predefined rules, MACEDON leverages LLMs for context-aware refinements. By integrating evaluation and optimization into a single interface, this approach moves beyond existing tools that treat these essential development steps as isolated processes.