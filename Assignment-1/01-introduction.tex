\section{Introduction}
The advent of Large Language Models (LLMs) has led to a paradigm shift in AI-driven code assistants \cite{chen2021evaluating,li2022competition,zan2023large,friedman2021introducing,zhang2023unifying} and brought transformative changes to programmers' workflow. Due to the lack of expertise, novice programmers heavily rely on LLM-driven code assistants in their workflow to generate or optimize sophisticated code from natural language (NL) prompts \cite{jiang2022discovering,mozannar2024reading,vaithilingam2022expectation}. Instead of manually reviewing their code, programmers can now declaratively express their optimization goals to LLMs. However, this approach inadvertently overlooks the need for programmers to understand their code in depth or to conduct goal-driven strategies for opti-mization, which could be challenging for novices when evaluating the quality of their generated code.

Recent research on programmers' interaction with LLM-driven code assistants has reflected these challenges for novice programmers. Specifically, uncertainty in generated code can lead to effi-ciency problems during the development process, as programmers are left to mentally anticipate possible outcomes until the code is generated. While understanding the current state of code is crucial for optimization prompts, there still remains a signifiacnt gap in supporting the complex pro-cess of forming optimization intentions. Therefore, our goal of this paper is explore a design that supports the iterative LLM driven code optimization and evaluation.

To address this gap, we conducted a study on 6 programmers of varying experience who use LLM based code optimization on a regular basis. Out aim was to realize how programmers come up with optimization strategies and the challenges they face. The study revealed the need of a \textit{structured, multidimensional} optimization and evaluation system that will also support selective segment optimization without affecting the whole codebase to a graet extent.

Gaining insight from the study, we focused on designing an LLM based code evaluation and optimization system based on the findings of the following research questions:
\begin{enumerate}[label=\textbf{RQ\arabic*-}, nosep]
    \item \textbf{Evaluation \& Strategy.} What evaluation dimensions and optimization strategies do programmers adopt when working with LLM-based assistants? 
    \item \textbf{Tool Design \& Effectiveness.} How can we design effective tools that support multi-dimensional code optimization, and to what extent are they useful for real-world programming tasks?
    \item \textbf{Generalizability.} Can such tools extend beyond novice programmers and remain effective in more diverse programming scenarios?  
\end{enumerate} 

To answer these RQs, we began by analyzing a large dataset of over 70,000 real-world code examples from the Performance Improving Edits (PIE) dataset to identify five measurable dimensions for code optimization. This analysis gave insights on \textbf{(RQ1)} by helping us understand how programmers evaluate and optimize their code. For anwsering \textbf{(RQ2)} we built MACEDON, a VSCode extension that provides multidimensional code evaluation, segment specific suggestions and optimization. In order to justify its effectiveness, we examined it on 24 novice programmers. The tool was built based on three derived design goals: 1) helping users assess their code across the five dimensions using interpretable scores, 2) offering specific suggestions such as improving loop efficiency or renaming unclear variables, and 3) enabling users to apply changes consistently with minimal manual effort.  Furthermore, to explore whether MACEDON is useful beyond novice programmers (RQ3), we conducted two case studies in which participants applied the tool to real programming tasks. In summary, our contribution is therefold:
\begin{itemize}[nosep]
\item A formative study and a comprehensive data analysis of optimization behaviors and needs when using LLMs for code evaluation and optimization. 
\item An interactive tool, MACEDON, developed as a Visual Studio Code Extension, that supports segment-specific code optimizations in multiple dimensions, minimizing rework and improving efficiency. 
\item A user study and two case studies for assessing MACEDON, demonstrating its effectiveness in improving code quality and user experience compared to conventional LLM-based assistants. 
\end{itemize}